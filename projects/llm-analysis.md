# Comparative Analysis of LLM's

Explored BERT and GPT by implementing both architectures from scratch and fine-tuning them for NLP tasks. Trained scaled-down versions on WikiText and fine-tuned on SQuAD, CNN/DailyMail datasets.

**Tech Stack**: Hugging Face, PyTorch, LLM's

**Key Achievements**:
- Implemented BERT and GPT architectures from scratch
- Fine-tuned models on SQuAD and CNN/DailyMail
- Achieved losses of 1.67 (BERT) & 2.97 (GPT)
- Created inference pipeline for model testing
- Comparative analysis of performance on various tasks

**[<i class="fa-solid fa-up-right-from-square"></i> View Project](https://github.com/tanayap/llm-analysis)**